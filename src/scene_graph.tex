In questo capitolo verrà affrontata la generazione del grafo di scena dato un frame e l'aggiornamento della Mappa Semantica con queste nuove informazioni per manterla aggiornata rispetto all'ambiente.
\begin{figure}[h]
	\includegraphics[width=\textwidth]{scene_graph/general_data_flow.png}
	\caption{Schema dei flussi dati per la generazione del grafo di scena e aggiornamento della mappa semantica }
\end{figure}

\section{Generazione del Grafo di Scena}
La generazione del grafo di scena è un passo fondamentale per il mantenimento della coerenza tra Mappa Semantica e Ambiente reale.\\
Il grafo di scena è una struttura dati che rappresenta gli oggetti presenti nell'ambiente e le relazioni tra loro composto da nodi e archi. I nodi rappresentano gli oggetti, mentre gli archi rappresentano le relazioni tra gli oggetti. \\
In questa sezione verrà affrontata la costruzione di questa struttura dati a partire da un frame RGB-D e il modello di ML utilizzato per l'inferenza degli oggetti e delle relazioni.
\subsection{Lettura del frame RGB-D}
All'interno dell'architettura cloud-native di Robee vi è la presenza di un pod chiamato "Streaming Module" il cui compito è streammare il feed video delle camera sul pod redis del robot, in modo che gli altri servizi o moduli possano accedere a questi dati tramite l'utilizzo di librerie wrapper, rendendo il tutto agnostico rispetto alla tipologia e al modello di videocamera utilizzati.

\subsection{Inferenza}
Ogni frame ricevuto dal feed video viene successivamente dato in input al modello PSGTr \cite{yang2022psg} che restituisce un oggetto di tipo Detections il quale contiene i seguenti dati:
\begin{itemize}
	\item labels: lista con lunghezza pari al numero di oggetti rilevati. Ogni valore indica la label corrispondente all' $i$-esimo oggetto. Per esempio, l'oggetto $i$-esimo ha label $labels[i]$;
	\item masks: lista contenente le maschere di ogni oggetto rilevato;
	\item bboxes: lista contenente le bounding boxes di ogni oggetto rilevato;
	\item rel\_pair\_idxes: lista con lunghezza pari al numero di relazioni tra oggetti rilevate. Ogni valore è a sua volta un array di dimensione due contenente gli indici dell'oggetto target e dell'oggetto sorgente della relazione;
	\item rel\_labels: lista con lunghezza pari al numero di relazioni tra oggetti rilevate. Ogni valore indica la label della $i$-esima relazione
	\item rel\_dists: lista con lunghezza pari al numero di relazioni tra oggetti rilevate. Ogni valore indica la probabilità associata alla $i$-esima relazione.
\end{itemize}
Questi dati vengono successivamente utilizzati per la costruzione del grafo di scena.

\subsubsection{Panoptic Scene Graph - Transformer}
Il modello PSGTr \cite{yang2022psg} è un modello di deep learning a singolo stato basato su architettura Transformer \cite{transformer} il cui obiettivo è quello di generare una rappresentazione a grafo della scena data la segmentazione panottica piuttosto che le bounding box degli oggetti rilevati.
\paragraph*{Training}
Il modello, per quanto riguarda gli oggetti, è stato addestrato su un dataset composto da 49mila immagini annotate basato su COCO \cite{coco} e Visual Genome \cite{visualgenemo}. Per le relazioni hanno estratto e costruito un dataset di 56 predicati a partire da dataset come VG-150 \cite{vg150}, VrR-VG \cite{vrvvg} and GQA \cite{cqa}.
\paragraph*{Segmentazione Panoptica}
La segmentazione panoptica individua gli oggetti e assegna a ogni pixel la label della classe dell'oggetto a cui appartengono. L'utilizzo di questa rispetto alle bounding da notevoli vantaggi:
\begin{itemize}
	\item Garantisce una localizzazione più precisa degli oggetti, segmentandoli a livello di pixel e riducendo la presenza di pixel rumorosi o ambigui tipici delle bounding box, che spesso includono porzioni di altre categorie o oggetti;
	\item Copre l'intera scena di un'immagine, inclusi gli sfondi, offrendo una comprensione più completa del contesto rispetto alle bounding box, che tendono a trascurare importanti informazioni di sfondo;
	\item Riduce anche le informazioni ridondanti o irrilevanti presenti nei dataset basati su bounding box, focalizzandosi sulla segmentazione degli oggetti piuttosto che sulle loro parti.
\end{itemize}
\paragraph*{Funzionamento di PSGTr}
L'architettura di PSGTr è basata su DETR \cite{detr} e HOI \cite{hoi}. Il modello predice triple $(soggetto, predicato, verbo)$ e la localizzazione degli oggetti simultaneamente.
\paragraph*{Pipeline PSGTr}
Attraverso una \gls{backbone} CNN, PSGTr estrae le features dell'immagine e i positional encodings che, insieme alle triplet queries, vengono dati in input al transformer encoder-decoder. In questo processo, l'obiettivo è che le query apprendano la rappresentazione del grafo di scena a triple in modo che per ognuna di esse, le predictions di  $(soggetto, predicato, verbo)$ possano successivamente essere estratte da tre Feed Forward Network. Infine, il task di segmentazione viene eseguito da due head panoptiche, una per il soggetto e una per l'oggetto della relazione.

\subsection{Costruzione del grafo}
L'obiettivo di questo step è la costruzione del grafo di scena rispetto all'ultimo frame. Per farlo, è necessario estrarre i dati dai risultati dell'inferenza di PSGTr e calcolare quei valori che dipendono dal sistema robot, come la posizione.
L'algoritmo di costruzione del grafo è costituito da 2 fasi principali:
\begin{itemize}
	\item Costruzione dei nodi per la scena semantica e per la mappa semantica:
	      \begin{itemize}
		      \item Calcolo della posizione dell'oggetto
	      \end{itemize}
	\item Costruzione degli archi per la scena semantica e per la mappa semantica
\end{itemize}

\subsubsection{Costruzione dei nodi}
\paragraph{Estrazione dati oggetto dai risultati}
L'oggetto MMDetResult ritornato dalla funzione di inferenza del modello, come detto precedentemente, possiede un attributo $labels$ che è una lista con lunghezza pari al numero di oggetti rilevati dove il valore $i$-esimo, indica l'indice della classe di appartenenza dell'oggetto $i$. Lo stesso meccanismo vale anche per le maschere.

\begin{algorithm}
	\caption{Estrazione classi e maschere degli oggetti individuati}
	\begin{algorithmic}[1]
		\State $obj\_classes \gets \text{[ ]}$
		\State $obj\_masks \gets \text{[ ]}$
		\State $obj\_labels\_ids \gets detectionResults.labels$
		\For{ $i=0$ to $obj\_labels\_ids.length$}
		\State $obj\_classes.append(PSG\_CLASSES[obj\_labels\_ids[i]])$
		\State $obj\_masks.append(detectionResults.masks[i])$
		\EndFor
	\end{algorithmic}
\end{algorithm}


\paragraph{Calcolo posizioni 3D}\label{paragraph:positions}
Per ogni oggetto, si estrae la posizione 3D nella mappa del robot in modo che questo possa successivamente localizzarlo e raggiungerlo.
\subparagraph{Calcolo posizione 3D nel \gls{pixel_system}}
Le maschere generate dal modello consentono di calcolare il centroide $(x_i, y_i)$ dell'oggetto $i$-esimo. Tuttavia, queste maschere forniscono solo un valore in due dimensioni. Per il calcolo del valore $z_i$ si utilizza il \Gls{point_cloud} che, combinando il frame RGB con il frame Depth, permette di ottenere una rappresentazione 3D della scena. \\
Per ogni oggetto $i$, si maschera il Point Cloud con la maschera $i$-esima e si calcola $z_i$ come valore mediano tra le $z_s$ di tutti i punti mascherati ottenendo cosí una posizione $P_{ipd} = (x_i, y_i, z_i)$.
% Reference System Image with panoptic example
\begin{figure}[h]
	\tdplotsetmaincoords{-10}{0}
	\begin{tikzpicture}[tdplot_main_coords]
		\begin{scope}[canvas is xy plane at z=0]
			\node[anchor=north west, inner sep=0] at (0,0) {\includegraphics[width=4cm]{scene_graph/segmentation_labeling.png}};
		\end{scope}

		% Draw the x-axis vector
		\draw[->, thick, red] (0,0,0) -- (3,0,0) node[anchor=north east]{$\mathbf{x}$};

		% Draw the y-axis vector
		\draw[->, thick, green] (0,0,0) -- (0,-2.8,0) node[anchor=north west]{$\mathbf{y}$};

		% Draw the z-axis vector
		\draw[->, thick, blue] (0,0,0) -- (1,0.5,-3) node[anchor=south]{$\mathbf{z}$};

		% Add labels for the origin
		\node at (0,0.4,0) [below left] {O};
	\end{tikzpicture}
	\caption{Sistema di coordinate pixel}
\end{figure}

\begin{algorithm}
	\caption{Calcolo della posizione 3D nel sistema pixel}
	\begin{algorithmic}[1]
		\Procedure{get\_pixel\_coords}{depth\_frame, obj\_mask}
		\State $z_{ip} \gets \text{median}(depth\_frame[obj\_mask])$ \Comment{Point Cloud}
		\State $x_{ip} \gets \text{median}(obj\_mask[:, 0])$
		\State $y_{ip} \gets \text{median}(obj\_mask[:, 1])$
		\State \Return $x_{ip}, y_{ip}, z_{ip}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subparagraph{Calcolo posizione 3D nel \gls{camera_system}}
Per ogni oggetto $i$, è necessario trasformare la posizione $P_{ipd}$ nel sistema di coordinate della camera, ovvero con la camera nell'origine. A tale scopo, si utilizza la \textbf{Matrice Intrinseca della Camera} ovvero la \gls{homogeneous_transformation_matrix} usata per convertire le coordinate in sistema camera a coordinate in sistema pixel. Questa dipende da caratteristiche fisiche della camera come apertura focale, campo visivo e risoluzione.\\
Poichè è necessario eseguire il procedimento inverso, ovvero trasformare le coordinate $P_{ipd}$ in sistema pixel a coordinate in sistema camera, viene utilizzata la Matrice Instrinseca Inversa e si esegue il prodotto matriciale tra questa e le coordinate $P_{ipd}$ aumentate, ottenendo così la posizione $P_{ic}$ degli oggetti in sistema camera.

\[
	\begin{bmatrix}
		x_{ic} \\
		y_{ic} \\
		z_{ic} \\
		1
	\end{bmatrix}
	=
	M_{ic}^{-1}
	*
	\begin{bmatrix}
		x_{ip} \\
		y_{ip} \\
		z_{ip} \\
		1
	\end{bmatrix}
\]

Dove:
\begin{itemize}
	\item $M_{ic}$ è la Matrice Intrinseca della Camera
	\item $x_{ip}, y_{ip}, z_{ip}$ sono le coordinate in sistema pixel dell'oggetto $i$
	\item $x_{ic}, y_{ic}, z_{ic}$ sono le coordinate in sistema camera dell'oggetto $i$
\end{itemize}

\begin{figure}[h]
	\tdplotsetmaincoords{-10}{0}
	\begin{tikzpicture}[tdplot_main_coords]
		\begin{scope}[canvas is xy plane at z=0]
			\node[anchor=center, inner sep=0, fill=white] at (0,0) {\includegraphics[width=4cm]{scene_graph/segmentation_labeling.png}};
		\end{scope}

		% Draw the x-axis vector
		\draw[->, thick, red] (0,0,0) -- (1,0.5,-2) node[anchor=south]{$\mathbf{x}$};

		% Draw the y-axis vector
		\draw[->, thick, green] (0,0,0) -- (-2, 0,0) node[anchor=north west]{$\mathbf{y}$};

		% Draw the z-axis vector
		\draw[->, thick, blue] (0,0,0) -- (0, 2, 0) node[anchor=south]{$\mathbf{z}$};

		% Add labels for the origin
		\node[] at (0,0.4,0) [below left] {\textcolor{white}{O}};
	\end{tikzpicture}
	\caption{Sistema di coordinate camera}
\end{figure}

\begin{algorithm}[h]
	\caption{Calcolo della posizione 3D nel sistema camera}
	\begin{algorithmic}[1]
		\Procedure{get\_camera\_coords}{$P_{ip}$}
		\State $M_{ic} \gets \Call{get\_camera\_intrinsics}{}$ \Comment{Funzione libreria Robee}
		\State $P_{ic} \gets M_{ic}^{-1} \times P_{ip}$
		\State $distance = norm(P_{ic} - P_{\text{camera}})$
		\State \Return $P_{ic}, distance$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subparagraph{Calcolo posizione 3D nel sistema mappa}
L'ultimo passaggio per ottenere la posizione dell'oggetto nella mappa è quello di utilizzare la Matrice Estrinseca della Camera, ovvero la \gls{homogeneous_transformation_matrix} usata per convertire le coordinate in sistema mondo a coordinate in sistema camera. Dipende dalla posizione e dall'orientamento della camera nel mondo. \\\\
Dato che in questo contesto la camera è montata sulla testa del robot, la matrice estrinseca dipende dalla posizione e dall'orientamento di quest'ultima e a seguire di tutte le trasformate nell'\gls{tf_tree} del robot. All'interno dell'architettura di Robee, sono presenti due moduli che si occupano di calcolare le trasformate per passare da un sistema di coordinate ad un altro. Si utilizzano dunque questi moduli per calcolare la matrice estrinseca inversa rispetto alla mappa/robot che moltiplicata per le coordinate $P_{ic}$ ottenute precedentemente, permette di ottenere la posizione dell'oggetto nel sistema mappa/robot.

\[
	\begin{bmatrix}
		x_{im} \\
		y_{im} \\
		z_{im} \\
		1
	\end{bmatrix}
	=
	M_{ec}^{-1}
	*
	\begin{bmatrix}
		x_{ic} \\
		y_{ic} \\
		z_{ic} \\
		1
	\end{bmatrix}
\]

Dove:
\begin{itemize}
	\item $M_{ec}$ è la Matrice Estrinseca della Camera
	\item $x_{ic}, y_{ic}, z_{ic}$ sono le coordinate in sistema camera dell'oggetto $i$
	\item $x_{im}, y_{im}, z_{im}$ sono le coordinate in sistema mappa dell'oggetto $i$
\end{itemize}

\begin{algorithm}
	\caption{Calcolo della posizione 3D nel sistema mappa}
	\begin{algorithmic}[1]
		\Procedure{get\_map\_coords}{$P_{ic}$}
		\State $M_{ec} \gets \Call{get\_camera\_extrinsics}{}$ \Comment{Funzione libreria Robee}
		\State $P_{im} \gets M_{ec}^{-1} \times P_{ic}$
		\State \Return $P_{im}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\paragraph{Istanziamento dei nodi}
Con tutti i dati ora a dispozione è possibile istanziare i nodi per la scena semantica e per la mappa semantica. Per la scena semantica, si istanziano i nodi con la posizione 3D nel sistema di riferimento del robot, mentre per la mappa semantica, si istanziano i nodi con la posizione 3D nel sistema di riferimento della mappa. \\
A causa del rumore del frame Depth nelle zone troppe vicine o troppo lontane dalla camera, è necessaria l'applicazione di un filtro per escludere gli oggetti che distano troppo dalla camera o che sono troppo vicini. Le soglie di distanza vengono impostate nei parametri di configurazione del servizio.

\begin{algorithm}
	\caption{Instanziamento dei nodi}
	\begin{algorithmic}[1]
		\State $obj\_ids \gets \text{[ ]}$
		\For{ $i=0$ to $obj\_labels\_ids.length$}
		\State $obj\_pixel\_coords \gets \Call{get\_pixel\_coords}{depth\_frame, obj\_masks[i]}$
		\State $obj\_camera\_coords, distance \gets \Call{get\_camera\_coords}{obj\_pixel\_coords}$
		\State $obj\_coords \gets \Call{get\_map\_coords}{obj\_camera\_coords}$
		\If{$distance > min\_distance$ \textbf{and} $distance < max\_distance$}
		\State $node \gets \text{Node}(i, obj\_classes[i], obj\_coords)$
		\State $obj\_ids.append(i)$
		\State $semantic\_scene.add\_node(node)$
		\State $semantic\_map.add\_node(node)$
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

\subsubsection{Costruzione degli archi}

\paragraph{Estrazione dati relazione dai risultati}
L'oggetto MMDetResult ritornato dalla funzione di inferenza del modello, come detto precedentemente, possiede gli attributi:
\begin{itemize}
	\item $rel\_labels$: lista con lunghezza pari al numero di relazione rilevate dove il valore $j$-esimo, indica l'indice della classe di appartenenza della relazione $j$.
	\item rel\_pair\_idxes: lista con lunghezza pari al numero di relazioni tra oggetti rilevate. Ogni valore è a sua volta un array di dimensione due contenente gli indici dell'oggetto target e dell'oggetto sorgente della relazione;
	\item $rel\_dist$: lista con lunghezza pari al numero di relazione rilevate dove il valore $j$-esimo, indica la probabilità associata alla relazione $j$.
\end{itemize}
È necessario estrarre questi dati e mettere in relazione gli oggetti tra loro per costruire gli archi del grafo.\\
Per ogni relazione $j$, si estrae l'indice dell'oggetto sorgente $s$ e l'indice dell'oggetto target $t$ e viene creato un arco tra i nodi corrispondenti se:
\begin{itemize}
	\item Entrambi i nodi siano presenti nella lista di oggetti precedentemente calcolata, ovvero rispettano i vincoli di distanza.
	\item La probabilità associata alla relazione sia maggiore di una certa soglia, impostata nei parametri di configurazione del servizio.
\end{itemize}

\begin{algorithm}
	\caption{Instanziamento degli archi}
	\begin{algorithmic}[1]
		\For{ $j=0$ to $rel\_labels.length$}
		\State $source\_idx \gets rel\_pair\_idxes[j][0]$
		\State $target\_idx \gets rel\_pair\_idxes[j][1]$
		\If{$source\_idx$ \textbf{in} $obj\_ids$ \textbf{and} $target\_idx$ \textbf{in} $obj\_ids$}
		\State $source\_node \gets semantic\_scene.get\_node(source\_idx)$
		\State $target\_node \gets semantic\_scene.get\_node(target\_idx)$
		\If{$rel\_dist[j] > rel\_threshold$}
		\State $semantic\_scene.add\_edge(source\_node, target\_node, rel\_labels[j])$
		\EndIf
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Aggiormento del Mappa Semantica}
In questa sezione verrà affrontato il processo di aggiornamento della Mappa Semantica con i nuovi dati ottenuti dalla Generazione del Grafo di Scena della sezione precedente.\\
La Mappa Semantica è una struttura dati che rappresenta l'ambiente circostante il robot e che viene aggiornata in real time per mantenere la coerenza con l'ambiente reale.\\
Il processo di aggiornamento della Mappa Semantica è composto da 3 fasi principali:
\begin{itemize}
	\item Trovare la stanza corrente del robot utilizzando il Grafo delle Stanze della Mappa Semantica
	\item Aggiornare il Grafo degli Oggetti della stanza corrente con i nuovi nodi e archi.
	\item Aggiornare la Mappa Semantica su DB e pubblicare i nuovi risultati sul relativo topic MQTT.
\end{itemize}

\subsection{Trovare la stanza corrente del robot}
Trovare la stanza corrente del robot è uno step necessario: permette di aggiornare il Grafo degli Oggetti della sola la stanza in cui il robot si trova, evitando di aggiornare l'intera mappa, efficientando il processo.\\
% is_inside_polygon
\subsubsection{Algoritmo di Ray Casting}
Ogni nodo stanza della Mappa Semantica ha un attributo $segments$ che rappresenta i segmenti che delineano i confini della stanza fisica. \\
Per determinare la stanza in cui si trova il robot, si utilizza l'algoritmo di Ray Casting \cite{haines_199424} che permette di determinare se un punto è all'interno di un poligono, il cui funzionamento è il seguente:
\begin{itemize}
	\item Si sceglie come punto di inizio $P_{start}$ del raggio un punto che sicuramente è all'esterno del poligono.
	\item Si emette un raggio dal punto $P_{start}$ al punto $P_{check}$ che si vuole verificare se è all'interno del poligono.
	\item Si contano quante volte il raggio interseca i segmenti del poligono:
	      \begin{itemize}
		      \item Se il numero è dispari, il punto è all'interno del poligono;
		      \item Se il numero è pari, il punto è all'esterno del poligono.
	      \end{itemize}
\end{itemize}
L'intuizione alla base di questo algoritmo è che se il segmento che congiunge il punto esterno e il punto $P_{check}$ interseca un numero dispari di segmenti del poligono significa che ho incontrato solo un "bordo" del poligono e quindi il punto è all'interno. Se il numero è pari, significa che ho incontrato due "bordi" e quindi sono o entrato e poi uscito dal poligono oppure nemmeno entrato.

\begin{figure}[h]
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\begin{tikzpicture}
			% Coordinate dei vertici del poligono
			\coordinate (A) at (0, 0);
			\coordinate (B) at (2, 3);
			\coordinate (C) at (4, 1);
			\coordinate (D) at (3, -2);
			\coordinate (E) at (1, -2);
			\coordinate (PS) at (0.5, 2);
			\coordinate (PCI) at (2, 0);

			% Disegna il poligono
			\draw[fill=blue!20] (A) -- (B) -- (C) -- (D) -- (E) -- cycle;
			\draw[dashed, name path=PSPCI] (PS) -- (PCI);
			\draw[name path=AB] (A) -- (B);
			% Etichetta i vertici
			\node at (A) [below left] {A};
			\node at (B) [above] {B};
			\node at (C) [right] {C};
			\node at (D) [below right] {D};
			\node at (E) [below] {E};
			\node at (PS) [above left] {$P_{start}$};
			\node at (PCI) [below right] {$P_{check}$};

			\foreach \point in {A, B, C, D, E}
			\fill (\point) circle (2pt);

			\foreach \point in {PS, PCI}
			\fill[blue] (\point) circle (2pt);

			\path[name intersections={of=PSPCI and AB, name=i}];
			\fill[red] (i-1) circle (2pt);
		\end{tikzpicture}
		\caption{Punto interno}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\begin{tikzpicture}
			% Coordinate dei vertici del poligono
			\coordinate (A) at (0, 0);
			\coordinate (B) at (2, 3);
			\coordinate (C) at (4, 1);
			\coordinate (D) at (3, -2);
			\coordinate (E) at (1, -2);
			\coordinate (PS) at (0.5, 2);
			\coordinate (PCO) at (4, -1);

			% Disegna il poligono
			\draw[fill=blue!20] (A) -- (B) -- (C) -- (D) -- (E) -- cycle;
			\draw[dashed, name path=PSPCO] (PS) -- (PCO);
			\draw[name path=AB] (A) -- (B);
			\draw[name path=CD] (C) -- (D);
			% Etichetta i vertici
			\node at (A) [below left] {A};
			\node at (B) [above] {B};
			\node at (C) [right] {C};
			\node at (D) [below right] {D};
			\node at (E) [below] {E};
			\node at (PS) [above left] {$P_{start}$};
			\node at (PCO) [below right] {$P_{check}$};

			\foreach \point in {A, B, C, D, E}
			\fill (\point) circle (2pt);

			\foreach \point in {PS, PCO}
			\fill[blue] (\point) circle (2pt);

			\path[name intersections={of=PSPCO and AB, name=first_i}];
			\fill[red] (first_i-1) circle (2pt);
			\path[name intersections={of=PSPCO and CD, name=second_i}];
			\fill[red] (second_i-1) circle (2pt);

		\end{tikzpicture}
		\caption{Punto esterno}
	\end{subfigure}
\end{figure}

\begin{algorithm}[h]
	\caption{Ray Casting}
	\begin{algorithmic}[1]
		\Procedure{is\_inside\_polygon}{$P_{start}, P_{check}, segments$}
		\State $intersection\_count \gets 0$
		\For{ $i=0$ to $segments.length$}
		\State $A \gets segments[i]$
		\State $B \gets segments[(i+1) \% segments.length]$
		\If{$\Call{do\_intersect}{P_{start}, P_{robot}, A, B}$}\Comment{Appendice A.2}
		\State $intersection\_count \gets intersection\_count + 1$
		\EndIf
		\EndFor
		\State \Return $intersection\_count \% 2 == 1$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection{Deteminare la stanza corrente}

Data la posizione del robot $P_{robot}$ nella mappa, per ogni stanza $s$ della Mappa Semantica si effettua l'algoritmo di Ray Casting sopra descritto dove:
\begin{itemize}
	\item $P_{start}$ viene scelto come un punto medio di un lato della bounding box con padding della stanza $s$;
	\item $P_{check}$ è la posizione del robot nella mappa.
\end{itemize}
Se il punto $P_{check}$, ovvero il robot, giace all'interno del poligono della stanza $s$, allora quest'ultima è la stanza corrente. \\
È bene notare che è impossibile che le stanze si sovrappongano, poichè sia il riconoscimento delle stanze (\textit{capitolo} \ref{chap:riconoscimento_stanze}) che la console per modificare la segmentazione effettuano un controllo per evitare questa casistica. Di conseguenza, l'algoritmo restituirà sempre una e una sola stanza.

\begin{algorithm}
	\caption{Trovare la stanza corrente}
	\begin{algorithmic}[1]
		\State $P_{robot} \gets \text{robot.get\_position()}$
		\State $current\_room \gets \text{None}$
		\For{ $s$ \textbf{in} $semantic\_map.get\_rooms()$}
		\State $bb\_x\_min, bb\_x\_max, bb\_x\_min \gets \Call{get\_bounding\_box}{s.segments}$
		\State $P_{start} \gets \text{[} \Call{mean}{bb\_x\_min, bb\_x\_max}, bb\_y\_min - \epsilon \text{]}$
		\If{$\Call{is\_inside\_polygon}{P_{start}, P_{robot}, s.segments}$}
		\State $current\_room \gets s$
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

\subsection{Aggiornamento Grafo degli Oggetti}
Una volta individuata la stanza corrente, si procede con l'aggiornamento del Grafo degli Oggetti della stanza corrente utilizzando i nuovi nodi e archi ottenuti dalla Generazione del Grafo di Scena.\\\\
Poichè il modello PSGTr individua e riconosce gli oggetti in modo non deterministico,  non assegna sempre lo stesso identificativo a un oggetto nei diversi frame. Questo comportamento rende difficile tracciare gli oggetti nel tempo. Di conseguenza, non ci si può basare su questi, tantomeno delle label a causa dei possibili omonimi, per aggiornare il grafo. La soluzione proposta è molto semplice:
\begin{itemize}
	\item Si proietta il Camera Frustum nel sistema mappa per determinare l'area della stanza attualmente visibile dalla telecamera;
	\item Si eliminano tutti i nodi oggetto dal Grafo degli Oggetti della stanza che giacciono all'interno del Camera Frustum e gli archi che entrano/escano da questi;
	\item Si aggiungono i nuovi nodi e archi ottenuti dall'ultima generazione del grafo di scena che rappresenta lo stato attuale della porzione di stanza visibile.
\end{itemize}

\subsubsection{Proiezione del Camera Frustum}
Il Camera Frustum è la porzione di spazio visibile dalla telecamera.
\begin{figure}[h]
	\centering
	\tdplotsetmaincoords{110}{110}
	\begin{tikzpicture}[tdplot_main_coords]
		% Define the coordinates of the frustum
		\coordinate (Camera) at (0, 0, 0);
		\coordinate (A) at (-2, 4, -2);
		\coordinate (B) at ( 2, 4, -2);
		\coordinate (C) at ( 2, 4,  2);
		\coordinate (D) at (-2, 4,  2);

		\coordinate (A') at (-0.40, 0.81, -0.4);
		\coordinate (B') at ( 0.40, 0.81, -0.4);
		\coordinate (C') at ( 0.40, 0.81,  0.4);
		\coordinate (D') at (-0.40, 0.81,  0.4);

		% Draw the frustum
		\draw[fill=green!20] (A) -- (B) -- (C) -- (D) -- cycle;
		\draw[fill=blue!20] (A') -- (B') -- (C') -- (D') -- cycle;

		\node at (A) [below right] {A};
		\node at (B) [below left] {B};
		\node at (C) [above left] {C};
		\node at (D) [above right] {D};

		\foreach \point in {A', B', C', D'}
		\draw[dashed] (Camera) -- (\point);

		\draw (A) -- (A');
		\draw (B) -- (B');
		\draw (C) -- (C');
		\draw (D) -- (D');

		% Draw the camera
		\fill[red] (Camera) circle (2pt);
		\node at (0,-0.5,0) [below] {Camera};

		% Draw the labels
		\foreach \point in {A, B, C, D, A', B', C', D'}
		\fill (\point) circle (2pt);

		\draw[->, thick, red]   (0,0,0) -- (0.6, 0,   0  ) node[anchor=north east]{$\mathbf{x}$};
		\draw[->, thick, green] (0,0,0) -- (0,   0.6, 0  ) node[anchor=north west]{$\mathbf{y}$};
		\draw[->, thick, blue]  (0,0,0) -- (0,   0,   0.6) node[anchor=south]{$\mathbf{z}$};

	\end{tikzpicture}
	\caption{Camera Frustum. I vertici del piano verde rappresentano la proiezione degli estremi del frame nel sistema di coordinate mappa.}
\end{figure}\\
Il funzionamento alla base di questo processo è molto simile a quello affrontato nella sezione \ref{paragraph:positions} per il calcolo della posizione 3D di un oggetto. I punti di partenza sono però i vertici del frame della camera e non le maschere degli oggetti. I vertici del frame dipendono dalla risoluzione della camera e sono:
\begin{itemize}
	\item $A = (0, 0)$
	\item $B = (width, 0)$
	\item $C = (width, height)$
	\item $D = (0, height)$
\end{itemize}
Dove $width$ e $height$ sono rispettivamente la larghezza e l'altezza del frame.\\
Per proiettare i vertici del frame nel sistema mappa, si procede come segue:
\begin{itemize}
	\item Gli estremi del frame vengono aumentati con $z=1$ e moltiplicati per la Matrice Intrinseca Inversa per ottenere i vettori $\vec{a}, \vec{b}, \vec{c}, \vec{d}$ nel sistema camera;
	\item I vettori $\vec{a}, \vec{b}, \vec{c}, \vec{d}$ vengono moltiplicati per la Matrice Estrinseca Inversa per ottenere i vettori $\vec{a'}, \vec{b'}, \vec{c'}, \vec{d'}$ nel sistema mappa.
	\item Per ogni vettore $\vec{v}$ del punto precedente:
	      \begin{itemize}
		      \item Viene calcolato il vettore direzione $\vec{d}$ come il vettore differenza tra $\vec{v}$ e $P_{camera}$ normalizzato:
		            \begin{equation}
			            \vec{d} = (\vec{v} - \overrightarrow{P_{camera}})/norm(\vec{v} - \overrightarrow{P_{camera}})
		            \end{equation}
		      \item Viene calcolato il punto finale $\vec{v'}$ come la somma tra il vettore normalizzato $\vec{d}$ moltiplicato per uno scalare $distance$ e $\overrightarrow{P_{camera}}$:
		            \begin{equation}
			            \vec{v'} = \vec{d} \cdot distance + \overrightarrow{P_{camera}}
		            \end{equation}
	      \end{itemize}
\end{itemize}
Questo processo consente di costruire un tetraedro con vertici $A'$, $B'$, $C'$, $D'$ e $P_{camera}$ che approssima il Camera Frustum nel sistema mappa che verrà utilizzato per eliminare gli oggetti del grafo che giacciono all'interno di esso in modo da poter aggiornare il grafo.
\begin{algorithm}
	\caption{Proiezione del Camera Frustum}
	\begin{algorithmic}[1]
		\Procedure{project\_camera\_frustum}{$P_{camera}, frame\_vertices$}
		\State $map\_vertices \gets$ [ ]
		\State $M_{ic} \gets \Call{get\_camera\_intrinsics}{}$
		\State $M_{ec} \gets \Call{get\_camera\_extrinsics}{}$
		\For{ $i=0$ to $frame\_vertices.length$}
		\State $\vec{v} \gets M_{ic}^{-1} \times frame\_vertices[i]$
		\State $\vec{v'} \gets M_{ec}^{-1} \times \vec{v}$
		\State $\vec{d} \gets (\vec{v'} - \overrightarrow{P_{camera}})$
		\State $\vec{d_n} \gets \vec{d} / norm(\vec{d})$
		\State $V \gets \vec{d_n} * distance + \overrightarrow{P_{camera}}$
		\State $map\_vertices.append(V)$
		\EndFor
		\State \Return $map\_vertices$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\subsubsection{Controllo della posizione degli oggetti}
Per stabilire se un oggetto giace nel frustum della camera e quindi è un oggetto da eliminare dal Grafo degli Oggetti della stanza corrente, si utilizza un algoritmo che coinvolge la costruzione dei piani del frustum e il controllo della posizione degli oggetti.
\paragraph{Costruzione dei piani}
Per definire un piano è necessario conoscere almeno tre punti $\vec{v_1}$,$\vec{v_2}$ e $\vec{v_3}$ in modo da poter calcolare la normale $\vec{n}$ e la distanza $d$ con segno:
\begin{equation}
	\begin{cases}
		\vec{n} = (\vec{v_1} - \vec{v_2}) \times (\vec{v_3} - \vec{v_2}) \\
		d = \vec{n} \cdot \vec{v_1} = \vec{n} \cdot \vec{v_2} = \vec{n} \cdot \vec{v_3}
	\end{cases}
\end{equation}
Il verso della normale del piano dipende dall'ordine dei vertici $\vec{v_1}$,$\vec{v_2}$ e $\vec{v_3}$: se i vertici sono disposti in senso orario, la normale punta verso l'interno del tetraedro, altrimenti punta verso l'esterno.\\
In questo processo, la normale deve necessariamente puntare verso l'interno del tetraedro.
Per esserne certi, si calcola il prodotto scalare tra la normale del piano e un vettore che sicuramente è all'interno del tetraedro, ad esempio il centroide $\vec{c}$ del solido. Se questo meno la distanza $d$ è minore di zero, allora la normale punta verso l'esterno del tetraedro e si negano i valori di $\vec{n}$ e $d$.
\begin{equation}
	\vec{n}  =
	\begin{cases}
		\vec{n}  & \text{se } \vec{n} \cdot \vec{c} - d \geq 0 \\
		-\vec{n} & \text{altrimenti}
	\end{cases} \;\;
	d        =
	\begin{cases}
		d  & \text{se } \vec{n} \cdot \vec{c} - d \geq 0 \\
		-d & \text{altrimenti}
	\end{cases}
\end{equation}
A supporto di queste operazioni, è stata definita una classe $Plane$ che permette di costruire un piano a partire da tre punti e di controllare se la normale punta verso l'interno del tetraedro.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\umlclass[x=4.3, y=0]{Plane}{
			+ normal : Vector \\
			+ d : float \\
		}{
			\tiny{$\ll$create$\gg$}\\
			+ \umlvirt{Plane}(v1 : Vector, v2 : Vector, v3 : Vector) \\
			+ checkNormal(center : Vector) : void \\
			\, - changeSign() : void \\
		}

	\end{tikzpicture}
	\caption{Classe Plane}
\end{figure}
I piani creati sono i seguenti:
\begin{itemize}
	\item Piano che passa per i vertici $\vec{d'}, \vec{c'}, \vec{b'}$;
	\item Piano che passa per i vertici $\overrightarrow{P_{camera}}, \vec{a'}, \vec{b'}$
	\item Piano che passa per i vertici $\overrightarrow{P_{camera}}, \vec{b'}, \vec{c'}$
	\item Piano che passa per i vertici $\overrightarrow{P_{camera}}, \vec{c'}, \vec{d'}$
	\item Piano che passa per i vertici $\overrightarrow{P_{camera}}, \vec{d'}, \vec{a'}$
\end{itemize}

\paragraph{Appartenenza di un punto al frustum}
%Controllare punto p dal piano%
Data la posizione dell'oggetto $P_i$ e i piani del camera frustum si controlla se per ognuno di questi l'oggetto giace nel sottospazio delimitato dal piano. Se l'oggetto giace nei sottospazi di tutti i piani allora l'oggetto è all'interno del frustum.\\\\
Sia $\vec{n}=(x_n,y_n,z_n)$ la normale del piano e $d$ la distanza con segno di un punto $\vec{p}$ dal piano. Il punto $\vec{p}=(x_p,y_p,z_p)$ giace nel sottospazio delimitato dal piano se:
\begin{equation}
	\vec{n} \cdot \vec{p} \geq d
\end{equation}
Che in cardinate cartesiane diventa:
\begin{equation}
	x_n \cdot x_p + y_n \cdot y_p + z_n \cdot z_p \geq d
\end{equation}

\begin{algorithm}
	\caption{Controllo appartenenza di punto al frustum}
	\begin{algorithmic}[1]
		\Procedure{is\_inside\_frustum}{$P_i, planes$}
		\State $inside \gets \text{True}$
		\For{ $i=0$ to $planes.length$}
		\State $\vec{n} \gets planes[i].normal$
		\State $d \gets planes[i].d$
		\If{$\vec{n} \cdot P_i < d$}
		\State $inside \gets \text{False}$
		\EndIf
		\EndFor
		\State \Return $inside$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\tdplotsetmaincoords{100}{90}
		\begin{tikzpicture}[tdplot_main_coords]
			% Define the coordinates of the frustum
			\coordinate (Camera) at (0, 0, 0);
			\coordinate (A) at (-2, 4, -2);
			\coordinate (B) at ( 2, 4, -2);
			\coordinate (C) at ( 2, 4,  2);
			\coordinate (D) at (-2, 4,  2);

			% Draw the frustum
			\draw (A) -- (B) -- (C) -- (D) -- cycle;


			% Draw the labels
			\foreach \point in {A, B, C, D}{
					\fill (\point) circle (2pt);
				}

			\foreach \point in {A, B, C}
			\draw (Camera) -- (\point);
			\draw[dashed] (Camera) -- (D);


			% Draw the camera
			\fill (Camera) circle (2pt);
			\node at (0,-0.5,0) [below] {Camera};

			% Draw the points
			\coordinate (Point1) at (-1, 3, -1);
			\coordinate (Point2) at (0, 2, 0);
			\coordinate (Point3) at (1, 1, 1);
			\coordinate (Point4) at (2, 0, 2);

			\fill[green] (Point1) circle (2pt);
			\fill[green] (Point2) circle (2pt);
			\fill[red] (Point3) circle (2pt);
			\fill[red] (Point4) circle (2pt);

		\end{tikzpicture}
		\caption{
			Prospettiva 1
		}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\tdplotsetmaincoords{100}{0}
		\begin{tikzpicture}[tdplot_main_coords]
			% Define the coordinates of the frustum
			\coordinate (Camera) at (0, 0, 0);
			\coordinate (A) at (-2, 4, -2);
			\coordinate (B) at ( 2, 4, -2);
			\coordinate (C) at ( 2, 4,  2);
			\coordinate (D) at (-2, 4,  2);

			% Draw the frustum
			\draw (A) -- (B) -- (C) -- (D) -- cycle;


			% Draw the labels
			\foreach \point in {A, B, C, D}{
					\fill (\point) circle (2pt);
					\draw[dashed] (Camera) -- (\point);
				}

			% Draw the camera
			\fill (Camera) circle (2pt);
			\node at (0.1,-0.1,0) [below right] {Camera};

			% Draw the points
			\coordinate (Point1) at (-1, 3, -1);
			\coordinate (Point2) at (0, 2, 0);
			\coordinate (Point3) at (1, 1, 1);
			\coordinate (Point4) at (2, 0, 2);

			\fill[green] (Point1) circle (2pt);
			\fill[green] (Point2) circle (2pt);
			\fill[red] (Point3) circle (2pt);
			\fill[red] (Point4) circle (2pt);

		\end{tikzpicture}
		\caption{
			Prospettiva 2
		}
	\end{subfigure}
	\caption{
		Controllo appartenenza di un punto al frustum. I punti verdi giacciono all'interno del frustum, i punti rossi all'esterno.
	}
\end{figure}

\subsubsection{Aggiornamento del Grafo}
Una volta proiettato il Camera Frustum e costruiti i piani, si procede con l'aggiornamento del Grafo degli Oggetti della stanza corrente.\\
Per ogni nodo oggetto $i$ del Grafo degli Oggetti della stanza corrente, si controlla se giace all'interno del frustum. Se è all'interno, allora si eliminano il nodo e gli archi che entrano/escano da esso. Se è all'esterno, si mantiene il nodo e gli archi che entrano/escano da esso.\\
Infine, si aggiungono i nuovi nodi e archi ottenuti dalla Generazione del Grafo di Scena della sezione precedente.

\begin{algorithm}[H]
	\caption{Aggiornamento del Grafo degli Oggetti}
	\begin{algorithmic}[1]
		\State $map\_vertices \gets \Call{project\_camera\_frustum}{P_{camera}, frame\_vertices}$
		\State $frustum\_planes \gets \Call{create\_frustum\_planes}{P_{camera},map\_vertices}$
		\State $room\_nodes \gets currentRoom.get\_nodes()$
		\State $room\_edges \gets currentRoom.get\_edges()$

		\State $nodes\_to\_remove \gets$ [ ] \Comment{Scelta nodi da rimuovere}
		\For{ $i=0$ to $room\_nodes.length$}
		\State $node \gets room\_nodes[i]$
		\If{$\Call{is\_inside\_frustum}{node.position, frustum\_planes}$}
		\State $nodes\_to\_remove.append(node)$
		\EndIf
		\EndFor

		\State $edges\_to\_remove \gets$ [ ] \Comment{Scelta archi da rimuovere}
		\For{ $i=0$ to $room\_edges.length$}
		\State $edge \gets room\_edges[i]$
		\If{$nodes\_to\_remove[edge.source] \textbf{or} nodes\_to\_remove[edge.target]$}
		\State $edges\_to\_remove.append(edge)$
		\EndIf
		\EndFor

		\For{ $i=0$ to $nodes\_to\_remove.length$} \Comment{Rimozione nodi e archi}
		\State $currentRoom.remove\_node(nodes\_to\_remove[i])$
		\EndFor
		\For{ $i=0$ to $edges\_to\_remove.length$}
		\State $currentRoom.remove\_edge(edges\_to\_remove[i])$
		\EndFor

		\State $current.add\_nodes(last\_scene\_graph.nodes)$ \Comment{Aggiunta nuovi risultati}
		\State $current.add\_edges(last\_scene\_graph.edges)$
	\end{algorithmic}
\end{algorithm}

\subsection{Salvataggio a DB e pubblicazione su MQTT}
Una volta aggiornato il Grafo degli Oggetti della stanza corrente, si procede con il salvataggio della Mappa Semantica aggiornata su DB e la pubblicazione dei nuovi risultati sul relativo topic MQTT.\\
Il salvataggio su DB è necessario per mantenere la coerenza della mappa tra le varie esecuzioni del sistema e condividere la conoscenza tra lo swarm di Robot che opera sulla stessa stanza. La pubblicazione su MQTT permette di rendere disponibili i nuovi risultati a tutti i moduli o servizi dell'architettura cloud di Robee.

\section{Conclusioni}
In questo capitolo è stata presentata la pipeline di generazione della Mappa Semantica attraverso l'utilizzo del modello PSGTr e l'aggiornamento di questa con i nuovi dati di inferenza del modello utilizzando tecniche geometriche.\\\\
~
Il modello PSGTr è stato scelto per la sua capacità di generare il grafo di scena in un tempo di inferenza ragionevole per applicazioni real time: richiede infatti circa $400ms$ in un cluster avente accesso a NVIDIA T4 permettendo così di aggiornare la rappresentazione semantica dell'ambiente praticamente instantaneamente.\\\\
~
La pipeline di aggiornamento della Mappa Semantica è semplice, efficiente ed efficace: Grazie all'utilizzo di operazioni di geometria lineare, che si traducono in moltiplicazioni e somme di numeri, è possibile aggiornare la mappa in un tempo ragionevole.\\\\
~
In futuro si può sperimentare introducendo modelli \cite{yang2023pvsg} che utilizzano anche il tracking degli oggetti segmentati in modo da poter tracciare gli oggetti nel tempo e aggiornare direttamente la mappa senza dover ricorrere a tecniche geometriche.

